{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077ff937-ac9a-43a2-a840-68f94579ba62",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Optimizer ::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3bd60-3b19-4044-a442-bd3b5c1f79a8",
   "metadata": {},
   "source": [
    "1) The Role and Necessity of Optimization Algorithms:\n",
    "\n",
    "Optimization algorithms in artificial neural networks (ANNs) are essential for training the networks to learn from data and improve their performance. The goal is to find the optimal set of parameters that minimize a given objective function or loss function. Optimization algorithms iteratively adjust the network's parameters based on the gradients of the loss function, gradually improving the model's performance.\n",
    "\n",
    "They are necessary because:\n",
    "- ANNs typically have a large number of parameters, making it infeasible to manually fine-tune them. Optimization algorithms automate the process of finding the optimal parameter values.\n",
    "- The loss function is often non-linear and highly complex, making it challenging to solve analytically. Optimization algorithms provide numerical techniques to search for optimal parameter values.\n",
    "- ANNs are trained on large datasets, and optimization algorithms enable efficient and scalable updates to the network parameters, making it feasible to handle big data.\n",
    "\n",
    "2) Gradient Descent and its Variants:\n",
    "\n",
    "Gradient Descent (GD) is a fundamental optimization algorithm used in machine learning, including ANNs. It works by iteratively updating the network's parameters in the direction opposite to the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "The basic steps of GD are as follows:\n",
    "- Compute the gradient of the loss function with respect to the parameters.\n",
    "- Update the parameters by subtracting a fraction of the gradient from the current parameter values, multiplied by a learning rate.\n",
    "\n",
    "Variants of gradient descent have been developed to address certain limitations of the basic GD algorithm:\n",
    "\n",
    "- Stochastic Gradient Descent (SGD): Instead of using the entire dataset to compute the gradient, SGD randomly selects a subset (mini-batch) of the data at each iteration. This reduces the computational cost per iteration but introduces more noise due to random sampling. SGD tends to have faster convergence speed than GD.\n",
    "\n",
    "- Batch Gradient Descent: This variant uses the entire dataset to compute the gradient at each iteration. It provides a more accurate estimate of the true gradient but can be computationally expensive, especially for large datasets.\n",
    "\n",
    "- Mini-batch Gradient Descent: It is a compromise between GD and SGD. It randomly selects a small batch of data (larger than SGD but smaller than the full dataset) to compute the gradient and update the parameters. Mini-batch GD balances the computational efficiency of SGD and the accuracy of GD.\n",
    "\n",
    "The trade-offs among these variants mainly involve convergence speed and memory requirements:\n",
    "\n",
    "- GD typically has slower convergence since it considers the entire dataset at each iteration but requires less memory as it does not store mini-batches.\n",
    "\n",
    "- SGD and mini-batch GD converge faster due to more frequent parameter updates but require memory to store mini-batches.\n",
    "\n",
    "- Batch GD has accurate gradient estimates but can be computationally expensive and memory-intensive due to considering the entire dataset.\n",
    "\n",
    "The choice of variant depends on the available computational resources, dataset size, and convergence speed requirements.\n",
    "\n",
    "Challenges with Traditional Gradient Descent Optimization Methods:\n",
    "\n",
    "Traditional gradient descent optimization methods, such as GD, face challenges that can hinder their effectiveness:\n",
    "- Slow Convergence: GD can be slow, especially for large datasets, as it computes the gradient using the entire dataset at each iteration. This results in high computational costs and slow updates to the parameters.\n",
    "\n",
    "- Local Minima: The loss function of ANNs is often non-convex, meaning it may have multiple local minima. Gradient descent methods can get stuck in suboptimal solutions if they converge to a local minimum instead of the global minimum.\n",
    "\n",
    "Modern optimizers address these challenges in different ways:\n",
    "\n",
    "- Adaptive Learning Rates: Modern optimizers dynamically adapt the learning rate based on the characteristics of the loss surface. This allows faster convergence by employing larger learning rates when the parameters are far from the optimal values and smaller learning rates as they approach the optimum.\n",
    "\n",
    "- Momentum: Momentum is a concept where the optimizer accumulates past gradients and utilizes their influence on the current parameter update. By introducing momentum, the optimizer can overcome flat regions in the loss surface, accelerate convergence, and avoid getting stuck in sharp minima.\n",
    "\n",
    "- Parameter Initialization Techniques: The initialization of network parameters can affect the convergence behavior of optimization algorithms. Modern optimizers often employ initialization techniques that help in finding better solutions and avoiding poor local minima.\n",
    "\n",
    "- Advanced Update Rules: Various update rules, such as Adam, RMSprop, AdaGrad, AdaDelta, etc., incorporate adaptive learning rates, momentum, and other techniques to improve convergence speed and avoid common optimization issues.\n",
    "\n",
    "3) These modern optimizers provide improvements in convergence speed, better handling of local minima, and the ability to handle large-scale datasets.\n",
    "\n",
    "Momentum and Learning Rate in Optimization Algorithms:\n",
    "\n",
    "Momentum and learning rate are crucial concepts in optimization algorithms. They impact convergence and model performance in the following ways:\n",
    "- Momentum: Momentum introduces an additional term to the parameter update, representing a fraction of the previous update. It helps accelerate convergence by accumulating the direction of past gradients and smoothing out fluctuations in the parameter updates. Higher momentum values allow the optimizer to have a stronger influence from past updates, leading to faster convergence. However, too high momentum can cause overshooting and oscillations around the optimum.\n",
    "\n",
    "- Learning Rate: The learning rate determines the step size taken in the direction of the gradient during parameter updates. A higher learning rate allows larger steps and faster convergence, but it can also cause overshooting, leading to divergence. On the other hand, a lower learning rate ensures smaller steps and better stability but may result in slow convergence. Finding an appropriate learning rate is crucial for balancing convergence speed and stability.\n",
    "\n",
    "Both momentum and learning rate are hyperparameters that need to be carefully tuned to achieve optimal performance in training ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b748145-0001-4eb6-a0f8-7f179906196e",
   "metadata": {},
   "source": [
    "# Part2: Optimizer Technique::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfab4a-0a1d-4903-b485-3ee09fc01ab9",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD):\n",
    "Stochastic Gradient Descent (SGD) is a variant of gradient descent that addresses some limitations of traditional gradient descent methods. Instead of computing the gradient using the entire dataset, SGD randomly selects a mini-batch of data at each iteration to estimate the gradient. This introduces noise in the gradient estimation but offers several advantages:\n",
    "Advantages of SGD:\n",
    "\n",
    "Faster Convergence: SGD often converges faster compared to traditional gradient descent methods. The more frequent updates based on mini-batches allow the optimizer to escape sharp minima and navigate through the loss surface more efficiently.\n",
    "\n",
    "Reduced Memory Requirement: Since SGD only uses a mini-batch of data, it requires less memory compared to batch gradient descent, making it more scalable for large datasets.\n",
    "\n",
    "Generalization: The noise introduced by random mini-batches in SGD helps in regularizing the model, preventing overfitting and improving generalization performance.\n",
    "\n",
    "Limitations and Suitable Scenarios for SGD:\n",
    "\n",
    "Noisy Gradient Estimates: The stochastic nature of SGD can introduce noise in the gradient estimation, which can lead to parameter updates that deviate from the optimal direction. This noise can make convergence less stable compared to batch gradient descent.\n",
    "\n",
    "Hyperparameter Sensitivity: SGD requires careful tuning of hyperparameters such as learning rate and mini-batch size. Inappropriate choices of these hyperparameters can lead to slow convergence or unstable training.\n",
    "\n",
    "SGD is particularly suitable in scenarios where large datasets are involved, as it allows efficient updates based on mini-batches. It is also useful when the computational resources are limited, as it requires less memory compared to batch gradient descent. However, proper hyperparameter tuning is crucial for obtaining good results with SGD.\n",
    "\n",
    "Adam Optimizer:\n",
    "Adam (Adaptive Moment Estimation) optimizer is an advanced optimization algorithm that combines the concepts of momentum and adaptive learning rates. It maintains an exponentially decaying average of past gradients and their squared values to adaptively adjust the learning rates for different parameters.\n",
    "The key components of the Adam optimizer are:\n",
    "\n",
    "Momentum: Adam incorporates a momentum term that accumulates the past gradients, similar to the concept of momentum in optimization algorithms. This helps in accelerating convergence and escaping sharp minima.\n",
    "\n",
    "Adaptive Learning Rates: Adam adapts the learning rate for each parameter based on their past gradients' first and second moments (mean and variance). It scales the learning rate based on the estimated variance, providing larger updates for parameters with smaller gradients and vice versa. This adaptive learning rate mechanism allows Adam to handle different types of gradients and learning rate requirements effectively.\n",
    "\n",
    "Benefits of Adam:\n",
    "\n",
    "Fast Convergence: Adam generally exhibits faster convergence compared to traditional gradient descent methods. The adaptive learning rates and momentum contribute to efficient updates and escaping poor minima.\n",
    "\n",
    "Robustness to Hyperparameters: Adam is known to be less sensitive to hyperparameter choices compared to other optimization algorithms. It performs well with a wide range of learning rates and momentum values, reducing the burden of extensive hyperparameter tuning.\n",
    "\n",
    "Handling Sparse Gradients: Adam performs well even when dealing with sparse gradients, as it adapts the learning rates based on the estimated variance of the gradients.\n",
    "\n",
    "Potential Drawbacks of Adam:\n",
    "\n",
    "Increased Memory Usage: Adam maintains additional variables to store the moving average of past gradients and squared gradients, resulting in increased memory requirements compared to simpler optimizers like SGD.\n",
    "\n",
    "Suboptimal Generalization: In some cases, Adam may have a tendency to overfit the training data, leading to suboptimal generalization performance on unseen data. Regularization techniques, such as weight decay, can be used to mitigate this issue.\n",
    "\n",
    "RMSprop Optimizer:\n",
    "RMSprop (Root Mean Square Propagation) optimizer is another optimization algorithm that addresses the challenges associated with adaptive learning rates. It computes an exponentially decaying average of past squared gradients for each parameter.\n",
    "Key features of the RMSprop optimizer include:\n",
    "\n",
    "Adaptive Learning Rates: RMSprop adaptively scales the learning rate for each parameter based on the historical squared gradients. It divides the learning rate by a running average of the magnitudes of recent gradients, effectively reducing the learning rate for parameters with frequent updates.\n",
    "\n",
    "Robustness to Sparse Gradients: Similar to Adam, RMSprop handles sparse gradients effectively by adapting the learning rate based on the magnitudes of recent gradients.\n",
    "\n",
    "Comparison of RMSprop and Adam:\n",
    "\n",
    "Similarities: Both RMSprop and Adam use adaptive learning rates to handle different gradients effectively. They are designed to address the issues of slow convergence and sensitivity to hyperparameters faced by traditional gradient descent methods.\n",
    "\n",
    "Differences: The main difference lies in how they estimate and utilize the historical gradients. Adam incorporates a momentum term and maintains both the first and second moments of gradients, while RMSprop only uses the second moment (squared gradients). Adam's momentum component helps in accelerating convergence and escaping sharp minima.\n",
    "\n",
    "Relative Strengths: Adam is known for its fast convergence, robustness to hyperparameters, and handling of different gradient types. RMSprop is efficient in handling sparse gradients and is computationally less expensive due to its simpler computation compared to Adam.\n",
    "\n",
    "Relative Weaknesses: Adam can be more memory-intensive compared to RMSprop due to the additional variables it maintains. RMSprop may not perform as well as Adam in scenarios where dealing with non-stationary objectives.\n",
    "\n",
    "The choice between RMSprop and Adam depends on the specific task, dataset, and computational resources available. It is often recommended to experiment with both optimizers and choose based on empirical performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf5fbd-ea7c-4f91-99e8-ec7f7bd5aa38",
   "metadata": {},
   "source": [
    "# Part3: Aplying Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a19188d-2085-4b9a-9ac9-9b3f342e97f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1599\n",
      "Number of columns: 12\n",
      "Categorical variables: Index(['quality'], dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 48000\n  y sizes: 1023\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39msgd_optimizer,\n\u001b[1;32m     41\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     42\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 5. Train the model with SGD optimizer\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 6. Compile and train the model with Adam optimizer\u001b[39;00m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39madam_optimizer,\n\u001b[1;32m     49\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     50\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1846\u001b[0m         label,\n\u001b[1;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1848\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1849\u001b[0m         ),\n\u001b[1;32m   1850\u001b[0m     )\n\u001b[1;32m   1851\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 48000\n  y sizes: 1023\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wine.csv')\n",
    "print(\"Number of rows:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])\n",
    "\n",
    "categorical_vars = df.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical variables:\", categorical_vars)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_vars)\n",
    "target_variable = df.columns[-1]\n",
    "features = df.drop(target_variable, axis=1)\n",
    "target = df[target_variable]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the number of neurons in each layer\n",
    "input_dim = X_train_scaled.shape[1]  # Number of features\n",
    "\n",
    "# 2. Define your model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Define optimizer configurations\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# 4. Compile the model\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Train the model with SGD optimizer\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 6. Compile and train the model with Adam optimizer\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 7. Compile and train the model with RMSprop optimizer\n",
    "model.compile(optimizer=rmsprop_optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 8. Evaluate model performance on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec846db-7ecd-428c-a9ef-8dd511dafa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
